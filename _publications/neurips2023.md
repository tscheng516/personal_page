---
title: "A Theoretical Analysis of the Test Error of Finite-Rank Kernel Ridge Regression"
collection: publications
permalink: /publication/neurips2023
excerpt: 'This paper is about the non-asymptotic approximation of test error in kernel method.'
date: 2009-10-01
venue: 'Neurips 2023'
paperurl: 'http://tscheng516.github.io/personal_page/files/paper_neurips2023.pdf'
citation: 'Cheng, Tin Sum, et al. "A Theoretical Analysis of the Test Error of Finite-Rank Kernel Ridge Regression." arXiv preprint arXiv:2310.00987 (2023).'
---

Abstract
=====
Existing statistical learning guarantees for general kernel regressors often yield loose bounds when used with finite-rank kernels. Yet, finite-rank kernels naturally appear in several machine learning problems, e.g. when fine-tuning a pre-trained deep neural networkâ€™s last layer to adapt it to a novel task when performing transfer learning. We address this gap for finite-rank kernel ridge regression (KRR) by deriving sharp non-asymptotic upper and lower bounds for the KRR test error of any finite-rank KRR. Our bounds are tighter than previously derived bounds on finite-rank KRR, and unlike comparable results, they also remain valid for any regularization parameters. 

[Download paper here](http://tscheng516.github.io/personal_page/files/paper_neurips2023.pdf)

Recommended citation: Cheng, Tin Sum, et al. "A Theoretical Analysis of the Test Error of Finite-Rank Kernel Ridge Regression." arXiv preprint arXiv:2310.00987 (2023). 
