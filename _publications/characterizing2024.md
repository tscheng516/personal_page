---
title: "Characterizing Overfitting in Kernel Ridgeless Regression Through the Eigenspectrum"
collection: publications
permalink: /publication/characterizing2024
excerpt: 'This paper is about overfitting in kernel method.'
date: 2024-02-05
venue: '-'
paperurl: 'http://tscheng516.github.io/personal_page/files/paper_characterizing2024.pdf'
citation: 'Tin Sum Cheng, Aurelien Lucchi, Anastasis Kratsios, and David Belius. Characterizing Overfitting in Kernel Ridgeless Regression Through the Eigenspectrum. arXiv preprint:2402.01297 , 2024. '
---

Abstract
=====
We derive new bounds for the condition number of kernel matrices, which we
then use to enhance existing non-asymptotic test error bounds for kernel ridgeless regression
in the over-parameterized regime for a fixed input dimension. For kernels with polynomial
spectral decay, we recover the bound from previous work; for exponential decay, our bound is
non-trivial and novel.
Our conclusion on overfitting is two-fold: (i) kernel regressors whose eigenspectrum decays
polynomially must generalize well, even in the presence of noisy labeled training data; these
models exhibit so-called tempered overfitting; (ii) if the eigenspectrum of any kernel ridge regressor
decays exponentially, then it generalizes poorly, i.e., it exhibits catastrophic overfitting.
This adds to the available characterization of kernel ridge regressors exhibiting benign overfitting
as the extremal case where the eigenspectrum of the kernel decays sub-polynomially.
Our analysis combines new random matrix theory (RMT) techniques with recent tools in the
kernel ridge regression (KRR) literature.

[Download paper here](http://tscheng516.github.io/personal_page/files/paper_characterizing2024.pdf)

Recommended citation: Tin Sum Cheng, Aurelien Lucchi, Anastasis Kratsios, and David Belius. Characterizing Overfitting in Kernel Ridgeless Regression Through the Eigenspectrum. arXiv preprint:2402.01297 , 2024.
